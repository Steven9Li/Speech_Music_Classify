\documentclass[11pt]{article}
\usepackage{parskip}
\usepackage{color}

\title{Discriminating speech and music with EEG}
\author{Nathaniel Zuk}

\begin{document}
\maketitle

\textit{This idea was inspired by Emily Teoh's work, and the analysis is based on her experiment and data.}

\textcolor{green}{(4/5/2018)}

Emily recorded EEG as subjects continuously listened to 2 second segments of different types of sounds.  She was interested in understanding how well the sounds could be classified (30 sounds in total) using the recorded EEG signal.  Using an LDA classifier she was able to classify sounds with a maximum accuracy of 18% and a minimum of 4.0%.  It appears that music and then speech had the highest classification accuracies (Figure 5.1).  Furthermore, music and speech had the highest phase dissimilarity, with a maximum in theta band (4-8 Hz).

In a separate experiment, she presented 1 second segments of 50 speech sounds and 50 music sounds and recorded EEG.  She then used an LDA classifier to discriminate speech and music from the EEG data.  Overall, speech and music are correctly classified around 62-63% and  of the time, which is above chance.

Overall, there appears to be signals in the EEG that are unique to speech and music.  Furthermore, they are unique within speech and music classes because their classification accuracy in the first experiment appeared to be higher than other sounds. 

So I want to know:

\begin{itemize}
\item Do the speech and music responses consistently rank highest in classification accuracy?
\item Does the EEG data cluster for speech and music?  It should since their classification accuracy is clearly higher.
\item What are the neural signatures for speech and music?  How do they different across speech and music types?
\item Can the EEG responses be predicted by subcortical processing?  Discrimination of speech and music at a subcortical level?
\end{itemize}

First, we should verify if speech and music classification is ranked highest across subjects.

Second, if speech and music are being classified best, then it suggests that those EEG signals will cluster into groups corresponding to the different types of speech and music stimuli.  We can assess this possibility of clustering using multidimensional scaling, which should find the dimensions that maximize the distances between the EEGs in each trial.  If they are clustering, it should show up here.

Last, we should look at the "template" neural signals for speech and music.  How is speech and music different?  How are these templates variable across speech and music stimuli?

\textcolor{green}{(4/12/2018)}

\textit{Speech / Music classification:}

My hypothesis is that speech and music stimuli will be classified better than other types of stimuli.  At least preliminarily, Teoh's confusion matrix suggests that the are classified better than the other stimuli.  This would imply that there's specialized encoding of speech and music measurable with EEG.

If they can be classified relative to other sounds, it suggests that their EEG signals might cluster in MDS.

\textbf{Analysis:} Load the EEG data for each subject, and train a classifier with 10-fold cross-validation on the 50 trials for each stimulus.  Then rank the testing performances.

Apply MDS on the data to see how separable the data are.

\textcolor{green}{(4/16/2018)}

On Friday (4/13) I spoke with Ed briefly about my recent results.  I found that the speech, music, and non-vocal human sounds rank significantly higher in classification accuracy across subjects that the other sounds.  This appears to be pretty consistent across subjects based on inspection of the confusion matrices (see ClassifySingleSubject.m).  Classification was performed with mcLDA using fitcdiscr.m, and the performance appeared to be comparable to what Emily Teoh found in her masters thesis.  These results suggest (and Ed agreed) that speech, music, and non-vocal human sounds are somehow "special" and uniquely encoded in the EEG.  It highlights the importance of using ecologically important sounds like speech and music to study the brain.

Ed and Emily wondered if these differences could be due to low-level statistics.  Can low-level statistics explain the specific improvement in classification accuracy?  For the non-vocal human sounds, they might.  Those sounds were "walking on a hard surface" and "chopping", both of which contain short and regular onsets which would probably have unique modulation spectra.  For speech and music, their low-level statistics might be different, but it's hard to tell if that's what's driving the differences in classification.  I would need to quantify the contribution of low-level statistics specifically.

One way of doing this would be to present "scrambled" sounds to subjects with identical long-term statistics.  This would require a new experiment where subjects listen to both the 2 second array of sounds as well as "scrambled" versions of those sounds.  For sound textures, synthesized versions with identical statistics but different seeds (aka. different waveforms) are hard to distinguish when they are longer than ~500 ms (McDermott et al, 2013, Figure 2).  If classification accuracy drops for the scrambled stimuli, then the classification accuracy differences are due to high-level processing of speech and music.

However, this identifies whether or not the classification arises from low-level statistics, but it doesn't allow us to identify which statistics are responsible.  In fact, given that only two seconds of the stimuli were presented, I expect low-level statistics to be important.  In Sam's study, he showed that scrambled music and speech with similar statistics produces a reduced response in the independent components in fMRI that are specific to speech and music, implying that those regions are "high-level" because they can't be explained by low-level processing.  There's no indication about what the high-level processing is though, just that it can't be explained by a low-level model.

Another option would be to examine how the classification varies with other types of speech and music stimuli.  Does the brain classify them equally?  If so, then when lots of different speech and music stimuli are presented, there should be no significant differences in classification accuracy for different stimulus sources.  If different time points of a single stimulus are presented (different parts of an audiobook with a single speaker, different parts of a song), the classification accuracy for those, would be lower, I would expect, than stimuli that are unique, since stimuli from the same source might be misclassified with other stimuli from that source.

If there are differences across stimuli, then we can rank the stimuli to identify the specific types of stimuli responsible for guiding the classification.  This might be a more powerful approach than the "original" vs "scrambled" classification, since that approach will only determine if the differences are due to low-level processing.  And classification of general sounds shows that there's something "special" about speech and music when EEG is encoding sounds.

\textcolor{blue}{Does classification accuracy change when only 1 second of the stimuli are included for the natural sounds classification?}

I used the data Emily collected where subjects listened to 1 second of speech and music clips from a variety of stimuli, and tried to classify the EEG responses to those.  It appears that there is no significant difference in classification accuracy for different sounds of the same type (speech or music).  The speech sounds are classified better than chance (determined because the diagonal has the highest classification accuracy, no statistical test performed besides that), but there's no difference between the stimuli.  For music it's not really classified better than chance and there's no significant difference between stimuli (Kruskal-Wallis across the diagonal).  There were only 6 subjects though for this experiment, so it may have been underpowered.

\textcolor{blue}{How do I visualize the EEG signals that are most responsible for classification accuracy and the improved classification of speech / music stimuli?}

Save the means of the gaussian distributions for each stimulus.  The relative distances between the means can be quantified using nonmetric multidimensional scaling (mdscale in Matlab).  With just 2 dimensions, the stimuli with the best classification performance appear to be in the outskirts (comparing the MDS plot to the confusion matrix).  It's possible that, once the distances are averaged across subjects, the distances between these distributions will be more stark.  I also may need to perform MDS directly on the EEG signal rather than the principal components in order to make it consistent across subjects.

\end{document}