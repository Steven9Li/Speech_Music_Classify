\documentclass[11pt]{article}
\usepackage{parskip}
\usepackage{color}
\usepackage{graphicx}

\title{EEG-based classification of natural sounds reveals specialized responses to speech and music}
\author{Nathaniel Zuk, Emily Teoh, Edmund Lalor}

\begin{document}
\maketitle

\textcolor{blue}{J Neurosci requirements:}

\begin{itemize}
\item Title = 50 words
\item Abbreviated title = 50 characters
\item Abstract = 250 words
\item Significance statement = 120 words
\item Introduction = 650 words
\item Discussion = 1500 words
\end{itemize}

\textbf{Abstract:}

\textbf{Introduction:}

\begin{itemize}
\item In previous work to study the processing of speech of music sounds, these sounds are resynthesized using vocoding or scrambling techniques in order to reduce the intelligibility of those sounds while maintaining simple aspects of their acoustic structure. Often, these studies show enhanced responses in EEG to the originals \textcolor{red}{(sources? Zoefel et al)}.  However, it is unclear whether this is a result of specialized processing in the brain or simply due to the \textcolor{red}{coherence of features} in the speech and music sounds.
\item In animals, there is a considerable amount of work showing that auditory cortex contains specialized responses to vocalization, and statistics of sounds most ecologically relevant for the animal.
\item Recent work in humans, using fMRI, has revealed regions of auditory cortex that are especially sensitive to speech and music sounds, compared to various other natural sounds.  The temporal properties of these responses, however, are less clear.
\item \textbf{Here we use classificiation-based analyses using EEG to identify specialized neural responses to speech and music sounds.  Furthermore, unlike other natural sounds that evoke unique temporal responses (such as impact sounds), these responses appear to respond to higher-level temporal organization of the speech and music sounds.  We also find temporal differences in the neural responses to these sounds: speech sounds can be classified using only early evoked responses following stimulus onset, while evoked responses appear to contain information sporadically throughout the 2 s interval of the stimuli.  Our results highlight the specialization of the brain to speech and music sounds and reveal potential temporal differences in the processing of these sounds.}
\end{itemize}

\textbf{Methods:}
\begin{itemize}
\item \textcolor{red}{Experiment 1:} Stimuli consisted of 30 different two-second-long sounds which produced the strongest fMRI responses in the independent components of activity found in a previous study \textcolor{red}{(Norman-Haignere et al, 2015)}.  In each trial, the sounds were presented consecutively such that each sound was repeated twice during a trial, such that there were 60 sounds in each trial, lasting a total of 2 minutes.  During the trial, subjects were asked to detect a sound that was identical to the previous sound (a "one-back" task) by hitting the spacebar.  Subjects listened to 50 trials.
\item \textcolor{red}{Experiment 2:} In this experiment, the stimuli consisted a subset of the speech sounds, music sounds, and five "impact sounds" that were identified as having high within-frequency-channel real modulation correlations as determined by a spectrotemporal model of auditory processing \textcolor{red}{(McDermott \& Simoncelli)}.  We then included synthesized versions of these sounds that contained identical spectrotemporal statistics as quantified by the model.  As in experiment 1, each sound was repeated twice in a trial, but subjeccts were asked to detect a sound that was identical to the sound before the previous one (a "two-back" task). There were 40 trials in total.
\item \textit{Preprocessing:} EEG data was recorded using a Biosemi 128-channel system recorded at 512 Hz sampling rate.  The 128 channels were referenced to the mastoids and then filtered between 1-40 Hz using a chebychev type II filter with a \textcolor{red}{0.75 to 60 Hz stopband}.  Eyeblinks were removed using FastICA \textcolor{red}{(Hyvarinen \& Oja)}.  Channels with variance greater than 3x the interquartile range plus the median across all 128 electrodes was replaced with a signal equal to the average of the three closest electrodes weighted by the inverse of their distance from the electrode.
\item \textit{Classification analysis:} Each trial of EEG data was spliced into two-second EEG recordings during the sound presentations, clips that occurred during a target sound were removed from further analysis, resulting in 90-100 EEG clips per stimulus for Experiment 1 and 70-80 EEG clips per stimulus for Experiment 2.  In order to reduce the dimensionality of the data, since there were far more dimensions (128 EEG channels x 256 time samples) than data samples for classification, PCA was applied to the spatiotemporal responses for each sound clip and components were retained that captured 95\% of the variance of the data (600-1200 components out of 32768 total dimensions).  The stimulus was then classified based on the principal components of the EEG data using multi-class linear discriminant analysis (\textit{fitcdiscr} in Matlab).  75\% of the data was randomly selected for training the classifier and the classifier was tested on the remainin 25\% of the data.  This selection was repeated 100 times to get 100 classification accuracies for the 30 different stimuli.  This analysis was repeated separately for each subject.  
\item \textcolor{green}{Figure showing all steps: experiment design and task, EEG classification, single-subject classification accuracies.}
\item \textcolor{red}{Population analyses:} To compare classification accuracies across subjects, the classification accuracies were ranked prior to population analysis.
\end{itemize}

\textbf{Results:}

\begin{itemize}
\item All stimuli were classified significantly better than chance \textcolor{green}{(Figure for classification accuracy for one subject?)}.  However, across the six subjects in Experiment 1, speech, music, and "impact" sound were classified significantly better than all other natural sounds.  The impact sounds were characterized by sharp acoustic onsets, likely evoking ERPs.  We could quantify these statistical attributes by looking at the withing-frequency-channel modulation correlations, for which the real values were notably higher for these two sounds than the other sounds in the dataset. \textcolor{green}{(Figure showing classification accuracy for natural sounds, rankings of classification accuracy, spectrograms of impact sounds)}
\item The improved speech and music classification could have also been a result of the particular statistics of those sounds, like the impact sounds.  However, prior work has shown that speech and music selectivity appears to be sensitive to acoustic features beyond spectrotemporal statistics \textcolor{red}{(Norman-Haignere et al)}.  To validate if it was a result of these lower-level features, we ran Experiment 2 which included speech, music, and impact sounds, as well as model-matched sounds that were generated to have the same spectral and modulation statistics as the originals \textcolor{red}{(McDermott \& Simoncelli)}.  We then repeated the classification analysis.  Across all 15 subjects that were recorded during experiment 2, we found that the model-matched impact sounds were classified identically to the originals, while the model-matched speech and music sounds were classified significantly worse than the originals \textcolor{green}{(Ranked classification accuracies for original vs synthetic)}.  This shows that the neural responses to speech and music producing the higher classification accuracies were responsive to acoustic features beyond the spectrotemporal ("low-level") statistics.
\item Next, we repeated the analysis of the EEG data in Experiment 2 using 200 ms blocks of time every 100 ms.  This allowed us to examine how classification accuracies and confusion between the stimuli change over the course of the stimulus.  Prior work has shown that confusion between images \textcolor{red}{(Cichy et al, 2014)} and phonemes \textcolor{red}{(Khaliginejad et al, 2017)} varies as a function of time, indicating different stages of processing for the stimuli.  However, for the stimuli used in Experiment 2 we found no confusion between classes of stimuli \textcolor{green}{(Plot classification accuracy over time)}.  This could be due to unique temporal responses for the different stimuli.
    \begin{itemize}
    \item \textcolor{red}{Cichy et al and others use classification when training on one time point and testing on another -- I might want to look at that.}
    \end{itemize}
\item We also found that classification accuracy for speech and music varies over time.  We compared the classification accuracies for the original speech and music to the average classification accuracies of the synthetic versions.  Speech shows significantly higher classification between 200-700 ms following stimulus onset \textcolor{green}{(Figure of time-varying classification accuracy)}.  Music similarly shows increased classification accuracy within this time range (specifically, 200-400 ms), but continues to show significantly high classification 1000-1200 ms and 1900 ms after stimulus onset.  \textcolor{blue}{This suggests differences in the temporal properties of the neural responses that can be used for classifying speech and music.}
\item We then examine classification channel-by-channel.  For all types of the original sounds, classification accuracies were highest in frontal channels.  The topography of these classification accuracies is indicative of auditory cortical activity as it is generated by a dipole from bilateral activation over auditory cortex \textcolor{red}{(Lalor et al)} \textcolor{green}{(Plot of topographies)}.
\end{itemize}

\textbf{Discussion:}

\begin{itemize}
\item Our results show that the specialized responses in the brain to speech and music stimuli are also temporally distinct.  This technique using EEG could be used to study how the brain responds to other types of sounds, and whether or not they evoke the speech and music selectivity in the brain.
\item This result differs from previous work in that it shows that the brain is especially responses to natural speech and music sounds.  We think this highlights the importance of studying brain processing using real speech and music recordings.
\item Temporally individualized responses to speech and music sounds because we observed little confusion between their EEG responses.
\item Modeling work suggests that at the level of the midbrain different populations of neurons may represent onsets for vowels in speech \textcolor{red}{(Carney et al)} and onsets for musical events.  This could represent an initial dichotomy of speech and music processing, giving rise to cortical specializations.  As such, we expected that these specialized temporal responses may make these sounds more classifiable in EEG.  Since we did observe this in EEG, it gives promise to identifying the underlying mechanisms responsible for producing these specializations.
\end{itemize}

\end{document}