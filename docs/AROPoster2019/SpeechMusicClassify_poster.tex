\documentclass[11pt]{article}
\usepackage{parskip}
\usepackage{color}

\title{Specialized High-Level Processing of Speech and Music Revealed with EEG}
\author{Nathaniel J Zuk, Emily S Teoh, Edmund C Lalor}

\begin{document}
\maketitle

\textbf{Introduction:}

Our brain can quickly identify different types of sounds, even when they are a couple of seconds long.  Prior work with fMRI identified regions of secondary auditory cortex that were especially responsive to speech sounds and music sounds, suggesting speech and music specialization in the cortex \textcolor{green}{[Norman-Haignere et al, 2015, Neuron]}. However, it is still unclear how speech and music are temporally processed for short stimuli, which could relate to early-stage dissociations between the processing of these two types of sounds. Classification-based analyses of EEG and MEG have successfully used to identify temporal processing stages for various stimuli \textcolor{green}{[Cichy et al, 2014, Nat Neurosci; Khalighinejad et al, 2017, J Neurosci]}.  Since neural responses in auditory cortex are sensitive to natural, ecological sounds \textcolor{green}{[Theunissen \& Elie, 2012, Nat Reviews Neurosci]}, we hypothesized that EEG might capture temporal activity neural specializations for speech and music in humans, despite is coarse spatial resolution.

\textbf{Here, by using classification-based measures, we demonstrate that these unique neural specializations are observable with EEG.}

\textbf{Experiment 1:}

\textit{Methods:} 

\begin{itemize}
\item 120 s, composed of 2-s clips of natural sounds, randomly arranged.  The natural sounds were categorized as: Environmental, Mechanical, Music, Non-speech vocal, Non-vocal human, Speech, and Animal sounds.  Subjects did a one-back detection task.
\item EEG was filtered with a chebyshev type II filter with a passband between 1 and 45 Hz with a 60 dB stopband at 0.75 Hz and 60 Hz.  Eyeblinks and electrical artifacts were removed with ICA.
\item Individual sound clips (100 recordings in total) were classified using regularized multi-class linear-discriminant analysis.  
\end{itemize}

\textit{Results:}

\begin{itemize}
\item All of the stimuli are classified greater than chance relative to the other stimuli.  However, the music and speech stimuli appeared to be classified better than all other environmental sounds.
\item When the classification accuracies for the different sounds were ranked for each subject, we found that the classification accuracies were significantly better for the music, speech, and "non-vocal human" sounds than all other natural sounds.
\item The non-vocal human sounds were "walking on a hard surface" and "chopping food", both of which are characterized as types of impact sounds.  Indeed, we found that those sounds had a clear acoustic marker that distinguished them from the other sounds in the set: a high real mod-C2 component.
\end{itemize}

\textcolor{red}{Additional plots?}

\begin{itemize}
\item Plot the stimuli as a function of average rank, to show which individual stimuli are classified best.
\end{itemize}

\textbf{Experiment 2:}

\textit{Methods:}

\begin{itemize}
\item Stimuli were setup identically to experiment 1, 120 s in total, 2-s sound clips.  Subjects performed a two-back task to ensure attention to all stimuli.
\item The sound clips consisted of 5 speech and 5 music sounds from the previous experiment, the 2 impact sounds from the previous experiment, and 3 more impact sounds with similarly high real mod-C2 statistics.  These sounds were then resynthesized with identical temporal and modulation statistics (McDermott & Simoncelli).
\end{itemize}

\textit{Results:}

\begin{itemize}
\item The classification accuracies for the original speech and music were significantly larger than the classification accuracies for the synthesized versions.  In contrast, the original impact sounds were classified no differently than their synthesized counterparts.
\item $\rightarrow$ The neural activity responsible for producing unique EEG responses to speech and music sounds results from processing stages beyond the low-level statistics of the sounds.
\end{itemize}

\textit{Topography and timing:}

\begin{itemize}
\item Classification using 200 ms intervals every 100 ms reveals that classification accuracy peaks for speech around 200-400 ms.  Music, however, remains significantly better than scrambled music classification nearly throughout the 2-second stimulus duration.
\item Topographies for the speech, music, and the impact sounds reveal that classification is best using frontal electrodes, suggesting auditory cortical activity and perhaps frontal activity are involved in responses necessary for classification.
\item \textcolor{red}{Should I be making a classification matrix by time (ala Cichy et al, 2014; Marti \& Dehaene, 2017)?}
\end{itemize}

\textbf{Conclusions and future work:}

\textcolor{red}{What can we learn from doing this work?  How is it different than Sam's?}

\begin{itemize}
\item Improved classification accuracy for speech and music sounds suggests that these responses are more pronounced than the neural responses to other types of sounds.  
\item These results highlight the improtance of using real speech and music, as they appear to produce more robust neural responses that other sounds and synthetic sounds with similar acoustics.
\item This technique demonstrates that EEG can be used to identify acoustics that activate neural processing region for speech and music. \textcolor{green}{[Norman-Haignere \& McDermott, 2018]}
\end{itemize}

\end{document}